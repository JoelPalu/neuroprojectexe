{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "23d06e892ef25e1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T16:36:30.051758Z",
     "start_time": "2025-05-02T16:36:24.021223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import shutil\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import ops\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers, Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.saving import register_keras_serializable\n",
    "from keras.models import load_model"
   ],
   "id": "474cd49d9cb34a64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teemu\\anaconda3\\envs\\keras\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Download",
   "id": "149a5e2b8f794e51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# RUN ONLY IF YOU DONT HAVE DATA SET YET\n",
    "\n",
    "urls = [\n",
    "    \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip\",\n",
    "    \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\"\n",
    "]\n",
    "\n",
    "data_dir = \"../loppuprojekti/data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "for url in urls:\n",
    "    filename = os.path.join(data_dir, os.path.basename(url))\n",
    "    # Download zip file\n",
    "    print(f\"Downloading {url}...\")\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    print(\"Download ready:\", filename)\n",
    "    # Exctract zip file\n",
    "    print(f\"Extracting {filename}...\")\n",
    "    with ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    print(\"Extracted files!\")\n",
    "\n",
    "print(\"Data pulling ready:\", data_dir)"
   ],
   "id": "ba7b7944112b4dcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation",
   "id": "ae802c879242de5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For GPU logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "data_dir = \"../loppuprojekti/data\"\n",
    "train = \"../loppuprojekti/data/DIV2K_train_HR/\"\n",
    "validation = \"../loppuprojekti/data/DIV2K_valid_HR/\"\n",
    "test = \"../loppuprojekti/data/DIV2K_test_HR\"\n",
    "\n",
    "os.makedirs(test, exist_ok=True)\n",
    "\n",
    "train_files = os.listdir(train)\n",
    "\n",
    "num_test_files = int(len(train_files) * 0.15)\n",
    "\n",
    "files_to_move = random.sample(train_files, num_test_files)\n",
    "\n",
    "if not os.listdir(test):\n",
    "    for file in files_to_move:\n",
    "        src = os.path.join(train, file)\n",
    "        dst = os.path.join(test, file)\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "    print(f\"Moved {len(files_to_move)} files to test directory\")\n",
    "\n",
    "else:\n",
    "    print(\"Test files already exist\")"
   ],
   "id": "5b62006901ca4a47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# NOT IN USAGE | tensorflow has this dataset already\n",
    "'''import os; os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "div2k_data = tfds.image.Div2k(config=\"bicubic_x4\")\n",
    "div2k_data.download_and_prepare()\n",
    "\n",
    "# Taking train data from div2k_data object\n",
    "train = div2k_data.as_dataset(split=\"train\", as_supervised=True)\n",
    "train_cache = train.cache()\n",
    "# Validation data\n",
    "val = div2k_data.as_dataset(split=\"validation\", as_supervised=True)\n",
    "val_cache = val.cache()'''"
   ],
   "id": "5d7ce1980d06e45c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data Augmentation Helper Functions",
   "id": "45bdcbb42d25fc3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def flip_left_right(lowres_img, highres_img):\n",
    "    \"\"\"Flips Images to left and right.\"\"\"\n",
    "\n",
    "    # Outputs random values from a uniform distribution in between 0 to 1\n",
    "    rn = keras.random.uniform(shape=(), maxval=1)\n",
    "    # If rn is less than 0.5 it returns original lowres_img and highres_img\n",
    "    # If rn is greater than 0.5 it returns flipped image\n",
    "    return ops.cond(\n",
    "        rn < 0.5,\n",
    "        lambda: (lowres_img, highres_img),\n",
    "        lambda: (\n",
    "            ops.flip(lowres_img),\n",
    "            ops.flip(highres_img),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def random_rotate(lowres_img, highres_img):\n",
    "    \"\"\"Rotates Images by 90 degrees.\"\"\"\n",
    "\n",
    "    # Outputs random values from uniform distribution in between 0 to 4\n",
    "    rn = ops.cast(\n",
    "        keras.random.uniform(shape=(), maxval=4, dtype=\"float32\"), dtype=\"int32\"\n",
    "    )\n",
    "    # Here rn signifies number of times the image(s) are rotated by 90 degrees\n",
    "    return tf.image.rot90(lowres_img, rn), tf.image.rot90(highres_img, rn)\n",
    "\n",
    "\n",
    "def random_crop(lowres_img, highres_img, hr_crop_size=96, scale=4):\n",
    "    \"\"\"Crop images.\n",
    "\n",
    "    low resolution images: 24x24\n",
    "    high resolution images: 96x96\n",
    "    \"\"\"\n",
    "    lowres_crop_size = hr_crop_size // scale  # 96//4=24\n",
    "    lowres_img_shape = ops.shape(lowres_img)[:2]  # (height,width)\n",
    "\n",
    "    lowres_width = ops.cast(\n",
    "        keras.random.uniform(\n",
    "            shape=(), maxval=lowres_img_shape[1] - lowres_crop_size + 1, dtype=\"float32\"\n",
    "        ),\n",
    "        dtype=\"int32\",\n",
    "    )\n",
    "    lowres_height = ops.cast(\n",
    "        keras.random.uniform(\n",
    "            shape=(), maxval=lowres_img_shape[0] - lowres_crop_size + 1, dtype=\"float32\"\n",
    "        ),\n",
    "        dtype=\"int32\",\n",
    "    )\n",
    "\n",
    "    highres_width = lowres_width * scale\n",
    "    highres_height = lowres_height * scale\n",
    "\n",
    "    lowres_img_cropped = lowres_img[\n",
    "        lowres_height : lowres_height + lowres_crop_size,\n",
    "        lowres_width : lowres_width + lowres_crop_size,\n",
    "    ]  # 24x24\n",
    "    highres_img_cropped = highres_img[\n",
    "        highres_height : highres_height + hr_crop_size,\n",
    "        highres_width : highres_width + hr_crop_size,\n",
    "    ]  # 96x96\n",
    "\n",
    "    return lowres_img_cropped, highres_img_cropped\n",
    "\n",
    "def PSNR(super_resolution, high_resolution):\n",
    "    \"\"\"Compute the peak signal-to-noise ratio, measures quality of image.\"\"\"\n",
    "    # Max value of pixel is 255\n",
    "    psnr_value = tf.image.psnr(high_resolution, super_resolution, max_val=255)[0]\n",
    "    return psnr_value"
   ],
   "id": "25577f9dc63d887d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Image Augmentation Main Function",
   "id": "197c0b9aecb5282c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dataset_object(ds, training=True):\n",
    "    ds = ds.map(\n",
    "        lambda lowres, highres: random_crop(lowres, highres, scale=4),\n",
    "        num_parallel_calls=AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    if training:\n",
    "        ds = ds.map(random_rotate, num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(flip_left_right, num_parallel_calls=AUTOTUNE)\n",
    "    # Batching Data\n",
    "    ds = ds.batch(16)\n",
    "\n",
    "    if training:\n",
    "        # Repeating Data, so that cardinality if dataset becomes infinte\n",
    "        ds = ds.repeat()\n",
    "    # prefetching allows later images to be prepared while the current image is being processed\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n"
   ],
   "id": "50835c5010d8ff5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Dataset Creation",
   "id": "9a1715567f09ea3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "crop_size = 96\n",
    "scale = 4\n",
    "\n",
    "train_dir = os.path.join(data_dir, \"DIV2K_train_HR\")\n",
    "val_dir = os.path.join(data_dir, \"DIV2K_valid_HR\")\n",
    "test_dir = os.path.join(data_dir, \"DIV2K_test_HR\")\n",
    "\n",
    "train_files = [\n",
    "    os.path.join(train_dir, fname)\n",
    "    for fname in os.listdir(train_dir)\n",
    "    if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "]\n",
    "\n",
    "val_files = [\n",
    "    os.path.join(val_dir, fname)\n",
    "    for fname in os.listdir(val_dir)\n",
    "    if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "]\n",
    "\n",
    "test_files = [\n",
    "    os.path.join(test_dir, fname)\n",
    "    for fname in os.listdir(test_dir)\n",
    "    if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def random_crop_and_downscale(hr_img):\n",
    "    hr_patch = tf.image.random_crop(hr_img, size=[crop_size, crop_size, 3])\n",
    "    hr_patch.set_shape([crop_size, crop_size, 3])\n",
    "    lr_patch = tf.image.resize(\n",
    "        hr_patch, [crop_size // scale, crop_size // scale], method=\"area\"\n",
    "    )\n",
    "    lr_patch.set_shape([crop_size // scale, crop_size // scale, 3])\n",
    "    return lr_patch, hr_patch\n",
    "\n",
    "\n",
    "\n",
    "# Build datasets\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_files).map(\n",
    "    load_image, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(val_files).map(\n",
    "    load_image, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_files).map(\n",
    "    load_image, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "\n",
    "# Apply random cropping and downscaling\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_sr = train_ds.map(random_crop_and_downscale, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_sr = train_sr.map(random_rotate, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_sr = train_sr.map(flip_left_right, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_sr = train_sr.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_sr = val_ds.map(random_crop_and_downscale, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_sr = val_sr.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_sr = test_ds.map(random_crop_and_downscale, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_sr = test_sr.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Example: print shapes\n",
    "\n",
    "for lr, hr in train_sr.take(1):\n",
    "    print(\"LR patch shape:\", lr.shape)\n",
    "    print(\"HR patch shape:\", hr.shape)"
   ],
   "id": "95e26c208752b61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Example of Training Data",
   "id": "8317d4b4d40d60b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Get one LR-HR pair from the dataset\n",
    "for lr, hr in train_sr.take(1):\n",
    "    lr_img = lr.numpy()\n",
    "    hr_img = hr.numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Low-Resolution\")\n",
    "    plt.imshow(lr_img[0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"High-Resolution\")\n",
    "    plt.imshow(hr_img[0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ],
   "id": "5504cd3e945bf0dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Utility Functions",
   "id": "3ff666c653a9df95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T16:37:27.029102Z",
     "start_time": "2025-05-02T16:37:27.023835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utilities\n",
    "@register_keras_serializable()\n",
    "class PixelShuffle(layers.Layer):\n",
    "    def __init__(self, scale, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.scale = scale\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.depth_to_space(inputs, block_size=self.scale)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"scale\": self.scale})\n",
    "        return config\n",
    "\n",
    "\n",
    "@register_keras_serializable()\n",
    "def PSNR(original, generated):\n",
    "    return tf.image.psnr(original, generated, max_val=1.0)\n",
    "\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    model.save(filepath)\n",
    "\n",
    "\n",
    "@register_keras_serializable()\n",
    "def load_model(filepath):\n",
    "    return keras.models.load_model(\n",
    "    filepath,\n",
    "    custom_objects={\n",
    "        \"PixelShuffle\": PixelShuffle,\n",
    "        \"combined_loss\": combined_loss,\n",
    "        \"PSNR\": PSNR,\n",
    "        \"SSIM\": SSIM,\n",
    "    },\n",
    ")\n",
    "\n",
    "@register_keras_serializable()\n",
    "def SSIM(y_true, y_pred):\n",
    "    return tf.image.ssim(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    # Preprocess for VGG\n",
    "    y_true_vgg = preprocess_vgg(y_true)\n",
    "    y_pred_vgg = preprocess_vgg(y_pred)\n",
    "\n",
    "    # Extract features\n",
    "    f_true = vgg_model(y_true_vgg)\n",
    "    f_pred = vgg_model(y_pred_vgg)\n",
    "\n",
    "    # Perceptual loss: feature-wise MSE\n",
    "    perceptual = tf.reduce_mean(tf.square(f_true - f_pred))\n",
    "    pixel = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "    return 0.01 * perceptual + 1.0 * pixel"
   ],
   "id": "9589d84e9e151c8a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Construction of Model",
   "id": "ec34495b9eb69c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # or val_calculate_psnr for PSNR\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "optimizer = Adam(\n",
    "    learning_rate=keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "        boundaries=[5000], values=[1e-4, 5e-5]\n",
    "    )\n",
    ")"
   ],
   "id": "7570775516dd8220"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Perceptual loss with VGG\n",
    "vgg = keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n",
    "vgg.trainable = False\n",
    "vgg_model = Model(inputs=vgg.input, outputs=vgg.get_layer(\"block3_conv3\").output)\n",
    "\n",
    "\n",
    "def preprocess_vgg(x):\n",
    "    x = tf.image.resize(x, [224, 224])\n",
    "    x = preprocess_input(x * 255.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Residual block layers\n",
    "def ResBlock(inputs):\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.Add()([inputs, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "# Up sampling layer\n",
    "def Upsampling(inputs, factor=2, **kwargs):\n",
    "    x = layers.Conv2D(64 * (factor**2), 3, padding=\"same\", **kwargs)(inputs)\n",
    "    x = PixelShuffle(factor)(x)\n",
    "    x = layers.Conv2D(64 * (factor**2), 3, padding=\"same\", **kwargs)(x)\n",
    "    x = PixelShuffle(factor)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Model creation function\n",
    "def make_model(num_filters, num_of_residual_blocks):\n",
    "    input_layer = layers.Input(shape=(None, None, 3))\n",
    "    x = layers.Rescaling(scale=1.0 / 255)(input_layer)\n",
    "    x = x_new = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "\n",
    "    for _ in range(num_of_residual_blocks):\n",
    "        x_new = ResBlock(x_new)\n",
    "\n",
    "    x_new = layers.Conv2D(num_filters, 3, padding=\"same\")(x_new)\n",
    "    x = layers.Add()([x, x_new])\n",
    "\n",
    "    x = Upsampling(x, factor=2)\n",
    "    x = layers.Conv2D(3, 3, padding=\"same\")(x)\n",
    "\n",
    "    output_layer = layers.Activation(\"sigmoid\")(x)\n",
    "    return Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = make_model(num_filters=64, num_of_residual_blocks=16)"
   ],
   "id": "e6689175239771ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.compile(optimizer=optimizer, loss=\"mae\", metrics=[PSNR, SSIM])\n",
    "model.fit(train_sr, validation_data=val_sr, epochs=50)\n",
    "model.save_weights(\"mae.weights.h5\")"
   ],
   "id": "ced1c1556d16838e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.load_weights('mae.weights.h5')\n",
    "model.compile(optimizer=optimizer, loss=combined_loss, metrics=[PSNR, SSIM])\n",
    "history = model.fit(train_sr, validation_data=val_sr, epochs=50, callbacks=[early_stop])\n",
    "model.save('super_resolution_model.keras')"
   ],
   "id": "aa94a53604083622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Bigger the PSNR value is, more prediction and original high res images are same\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['psnr'], label='Train PSNR')\n",
    "plt.plot(history.history['val_psnr'], label='Validation PSNR')\n",
    "plt.title('Model PSNR')\n",
    "plt.ylabel('PSNR')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "id": "34e1a77cf79c1399"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Test\n",
    "\n",
    "Testing model with full size bad quality image"
   ],
   "id": "868881d34e649ea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "N = 7  # Index for image to take\n",
    "for i, hr_img in enumerate(test_ds):\n",
    "    if i == N:\n",
    "        hr_img = hr_img.numpy()\n",
    "        break\n",
    "\n",
    "# Convert image to bad quality\n",
    "scale = 6\n",
    "h, w = hr_img.shape[0], hr_img.shape[1]\n",
    "lr_img = tf.image.resize(hr_img, [h // scale, w // scale], method=\"area\")\n",
    "lr_img_blocky = tf.image.resize(lr_img, [h, w], method=\"nearest\").numpy()\n",
    "\n",
    "# Predict image\n",
    "lr_input = tf.expand_dims(lr_img, axis=0)\n",
    "sr_img = model.predict(lr_input)[0]\n",
    "\n",
    "# Convert for display\n",
    "hr_img_disp = np.clip(hr_img * 255, 0, 255).astype(\"uint8\")\n",
    "sr_img_disp = np.clip(sr_img * 255, 0, 255).astype(\"uint8\")\n",
    "lr_img_disp = np.clip(lr_img_blocky * 255, 0, 255).astype(\"uint8\")\n",
    "\n",
    "# Show predictions\n",
    "plt.figure(figsize=(18, 18))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title(\"Low-Resolution (Native Size)\", fontsize=20)\n",
    "plt.imshow(np.clip(lr_img.numpy() * 255, 0, 255).astype(\"uint8\"), interpolation='nearest')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title(\"Super-Resolution\", fontsize=20)\n",
    "plt.imshow(sr_img_disp)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title(\"High-Resolution (Original)\", fontsize=20)\n",
    "plt.imshow(hr_img_disp)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "93035aec0212a5c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model demo",
   "id": "259eeede9ed33346"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T18:01:30.055279Z",
     "start_time": "2025-05-02T18:01:29.321184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TÄÄ ON WORK IN PROGRESS\n",
    "\n",
    "saved_model = load_model(\"super_resolution_model.keras\")\n",
    "\n",
    "def superRes(input_img):\n",
    "\n",
    "    input_img = input_img.astype(np.float32) / 255.0\n",
    "    img_array = np.expand_dims(input_img, axis=0)\n",
    "    sr_img = saved_model.predict(img_array)\n",
    "    sr_img = np.squeeze(sr_img, axis=0)\n",
    "    sr_img = np.clip(sr_img * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return sr_img\n",
    "\n",
    "demo = gr.Interface(fn=superRes, inputs=gr.Image(type=\"numpy\"), outputs=\"image\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ],
   "id": "a8259c28101fa925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
